{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11861194,"sourceType":"datasetVersion","datasetId":7147512},{"sourceId":11877962,"sourceType":"datasetVersion","datasetId":7453309}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Author: Mindy Ng, Markela Zineli","metadata":{}},{"cell_type":"markdown","source":"# 5. Evaluate whether LLMs try to help with situations they explicitly say they can’t handle\n\nPURPOSE: Arguably the most relevant to safety of all of the experiments. Both ChatGPT and Claude suggested that LLMs must provide disclaimers in order to reduce risks of people using LLMs for therapy. This experiment will evaluate whether they do provide these disclaimers for all crises that they say they can’t help with, and if they try to give advice for them (even though they said they can’t help with them).\n\n**Claude and ChatGPT said they can’t help users with these crises:**\n\nFrom Claude:\n\n- Active suicidal thoughts or plans\n- Severe self-harm behaviors\n- Acute psychosis or loss of touch with reality\n- Substance abuse crises or withdrawal\n- Acute panic attacks or severe anxiety episodes happening right now\n- Traumatic events that have just occurred\n- Severe depression affecting basic functioning\n- Eating disorders with medical complications\n- Bipolar disorder in manic or severely depressed states\n- Post-traumatic symptoms causing significant distress\n\nFrom ChatGPT:\n\n- **Suicidal thoughts or plans**\n- **Thoughts of self-harm or harming others**\n- **Psychosis (e.g., hearing voices, delusions)**\n- **Severe depression or panic that interferes with basic functioning**\n- **Substance abuse with significant life disruption**\n- **Trauma that’s causing flashbacks or dissociation**\n- **Eating disorders with medical complications**\n\nThings ChatGPT says it can help with:\n\n- Stress and burnout\n- Mild to moderate anxiety or depression\n- Relationship conflicts\n- Low self-esteem or motivation issues\n- Coping strategies for difficult emotions\n- Building healthy habits, like sleep and mindfulness\n\n1. Explicitly tell the LLMs that we are experiencing the above ^, and see if they outright tell us to seek professional help and don’t try to offer us advice.\n2. Create a dilemma for each of these mental health crises, and see if LLMs try to help even though they say they can’t help with these\n\n2 runs\n\nPrompt 1: text (scenario)\n\nPrompt 2: “Can you help me with something? + text (scenario)”","metadata":{}},{"cell_type":"code","source":"!pip install anthropic --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T20:30:30.427082Z","iopub.execute_input":"2025-05-21T20:30:30.427862Z","iopub.status.idle":"2025-05-21T20:30:34.247891Z","shell.execute_reply.started":"2025-05-21T20:30:30.427823Z","shell.execute_reply":"2025-05-21T20:30:34.246906Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nfrom openai import OpenAI\nimport anthropic\n\nimport numpy as np\nimport pandas as pd\n\nimport time\nfrom tqdm.auto import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-21T20:30:34.249729Z","iopub.execute_input":"2025-05-21T20:30:34.250024Z","iopub.status.idle":"2025-05-21T20:30:34.255402Z","shell.execute_reply.started":"2025-05-21T20:30:34.249998Z","shell.execute_reply":"2025-05-21T20:30:34.254392Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/ethics-therapy-probe-experiments/probe_experiments/experiment_5.csv\")\ndf = df[['crisis', 'issue', 'text']]\ndf","metadata":{"execution":{"iopub.status.busy":"2025-05-21T20:30:34.256324Z","iopub.execute_input":"2025-05-21T20:30:34.256598Z","iopub.status.idle":"2025-05-21T20:30:34.306793Z","shell.execute_reply.started":"2025-05-21T20:30:34.256578Z","shell.execute_reply":"2025-05-21T20:30:34.305935Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['text'][1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T20:30:34.307489Z","iopub.execute_input":"2025-05-21T20:30:34.307697Z","iopub.status.idle":"2025-05-21T20:30:34.314691Z","shell.execute_reply.started":"2025-05-21T20:30:34.307681Z","shell.execute_reply":"2025-05-21T20:30:34.313664Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Setup API keys\nuser_secrets = UserSecretsClient()\n\nlambda_key = user_secrets.get_secret(\"lambda\")\n\nopenai_key = user_secrets.get_secret(\"openai_apikey_apartlab\")\nanthropic_key = user_secrets.get_secret(\"anthropic_apartlab\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T20:30:34.316773Z","iopub.execute_input":"2025-05-21T20:30:34.317047Z","iopub.status.idle":"2025-05-21T20:30:34.565827Z","shell.execute_reply.started":"2025-05-21T20:30:34.317026Z","shell.execute_reply":"2025-05-21T20:30:34.564968Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Setup clients\nlambda_client = OpenAI(\n    api_key=lambda_key,\n    base_url=\"https://api.lambda.ai/v1\",\n)\n\nopenai_client = OpenAI(api_key=openai_key)\nanthropic_client = anthropic.Anthropic(api_key=anthropic_key)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T20:30:34.566605Z","iopub.execute_input":"2025-05-21T20:30:34.566852Z","iopub.status.idle":"2025-05-21T20:30:34.761207Z","shell.execute_reply.started":"2025-05-21T20:30:34.566832Z","shell.execute_reply":"2025-05-21T20:30:34.760164Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function for Lambda models (Llama and DeepSeek)\ndef process_lambda_models(df):\n    \n    # Models to process\n    lambda_models = [\"llama3.1-405b-instruct-fp8\", \"deepseek-v3-0324\"]\n\n    # Avoid pandas errors\n\n    df = df.copy()\n    \n    # Initialize columns for each model\n    for model_name in lambda_models:\n        safe_name = model_name.replace(\"-\", \"_\").replace(\".\", \"_\")\n        df.loc[:, f\"{safe_name}_text_response\"] = \"\"\n        df.loc[:, f\"{safe_name}_help_text_response\"] = \"\"\n    \n    # Process each row\n    for idx in tqdm(range(len(df)), desc=\"Processing Lambda models\"):\n        text_prompt = f\"{df.loc[idx, 'text']}\"\n        help_text_prompt = f\"Can you help me with something? {df.loc[idx, 'text']}\"\n        \n        # Process each model\n        for model_name in lambda_models:\n            safe_name = model_name.replace(\"-\", \"_\").replace(\".\", \"_\")\n            \n            try:\n                # Process scenario prompt\n                text_response = lambda_client.chat.completions.create(\n                    model=model_name,\n                    messages=[{\"role\": \"user\", \"content\": text_prompt}]\n                )\n                df.loc[idx, f\"{safe_name}_text_response\"] = text_response.choices[0].message.content\n                \n                time.sleep(1)  # Rate limiting\n                \n                # Process help text + scenario prompt\n                help_text_response = lambda_client.chat.completions.create(\n                    model=model_name,\n                    messages=[{\"role\": \"user\", \"content\": help_text_prompt}]\n                )\n                df.loc[idx, f\"{safe_name}_help_text_response\"] = help_text_response.choices[0].message.content\n                \n                time.sleep(1)  # Rate limiting\n                \n            except Exception as e:\n                print(f\"Error with {model_name} on row {idx}: {e}\")\n    \n    return df\n    \nllama_deepseek = process_lambda_models(df)\nllama_deepseek.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T20:30:34.763408Z","iopub.execute_input":"2025-05-21T20:30:34.763718Z","iopub.status.idle":"2025-05-21T20:39:46.719868Z","shell.execute_reply.started":"2025-05-21T20:30:34.763688Z","shell.execute_reply":"2025-05-21T20:39:46.718897Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"llama_deepseek['llama3_1_405b_instruct_fp8_text_response'][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T20:41:50.756536Z","iopub.execute_input":"2025-05-21T20:41:50.756817Z","iopub.status.idle":"2025-05-21T20:41:50.763630Z","shell.execute_reply.started":"2025-05-21T20:41:50.756798Z","shell.execute_reply":"2025-05-21T20:41:50.762477Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function for OpenAI model\ndef process_openai_model(df):\n   \n    model_name = \"chatgpt-4o-latest\"\n    safe_name = model_name.replace(\"-\", \"_\").replace(\".\", \"_\")\n\n    # Avoid pandas errors\n    df = df.copy()\n    \n    # Initialize columns\n    df.loc[:, f\"{safe_name}_text_response\"] = \"\"\n    df.loc[:, f\"{safe_name}_help_text_response\"] = \"\"\n    \n    # Process each row\n    for idx in tqdm(range(len(df)), desc=\"Processing OpenAI model\"):\n        text_prompt = f\"{df.loc[idx, 'text']}\"\n        help_text_prompt = f\"Can you help me with something? {df.loc[idx, 'text']}\"\n        \n        try:\n            # Process scenario prompt\n            text_response = openai_client.chat.completions.create(\n                model=model_name,\n                messages=[{\"role\": \"user\", \"content\": text_prompt}]\n            )\n            df.loc[idx, f\"{safe_name}_text_response\"] = text_response.choices[0].message.content\n            \n            time.sleep(1)  # Rate limiting\n            \n            # Process help text + scenario prompt\n            help_text_response = openai_client.chat.completions.create(\n                model=model_name,\n                messages=[{\"role\": \"user\", \"content\": help_text_prompt}]\n            )\n            df.loc[idx, f\"{safe_name}_help_text_response\"] = help_text_response.choices[0].message.content\n            \n            time.sleep(1)  # Rate limiting\n            \n        except Exception as e:\n            print(f\"Error with {model_name} on row {idx}: {e}\")\n    \n    return df\n    \ngpt_4o = process_openai_model(df)\ngpt_4o.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T20:41:55.269565Z","iopub.execute_input":"2025-05-21T20:41:55.269842Z","iopub.status.idle":"2025-05-21T20:44:03.814214Z","shell.execute_reply.started":"2025-05-21T20:41:55.269823Z","shell.execute_reply":"2025-05-21T20:44:03.813197Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gpt_4o['chatgpt_4o_latest_text_response'][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T20:44:09.217188Z","iopub.execute_input":"2025-05-21T20:44:09.217476Z","iopub.status.idle":"2025-05-21T20:44:09.223478Z","shell.execute_reply.started":"2025-05-21T20:44:09.217455Z","shell.execute_reply":"2025-05-21T20:44:09.222454Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function for Claude model \ndef process_claude_model(df):\n   \n    model_name = \"claude-3-7-sonnet-20250219\"\n    safe_name = model_name.replace(\"-\", \"_\").replace(\".\", \"_\")\n\n    # Avoid pandas errors\n    df = df.copy()\n    \n    # Initialize columns\n    df.loc[:, f\"{safe_name}_text_response\"] = \"\"\n    df.loc[:, f\"{safe_name}_help_text_response\"] = \"\"\n    \n    # Process each row\n    for idx in tqdm(range(len(df)), desc=\"Processing Claude model\"):\n        text_prompt = f\"{df.loc[idx, 'text']}\"\n        help_text_prompt = f\"Can you help me with something? {df.loc[idx, 'text']}\"\n        \n        try:\n            # Process scenario prompt\n            text_response = anthropic_client.messages.create(\n                model=model_name,\n                max_tokens=1000,\n                messages=[{\"role\": \"user\", \"content\": text_prompt}]\n            )\n            df.loc[idx, f\"{safe_name}_text_response\"] = text_response.content[0].text\n            \n            time.sleep(1)  # Rate limiting\n            \n            # Process help text + scenario prompt\n            help_text_response = anthropic_client.messages.create(\n                model=model_name,\n                max_tokens=1000,\n                messages=[{\"role\": \"user\", \"content\": help_text_prompt}]\n            )\n            df.loc[idx, f\"{safe_name}_help_text_response\"] = help_text_response.content[0].text\n            \n            time.sleep(1)  # Rate limiting\n            \n        except Exception as e:\n            print(f\"Error with {model_name} on row {idx}: {e}\")\n    \n    return df\n    \nclaude = process_claude_model(df)\nclaude","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T20:44:48.597787Z","iopub.execute_input":"2025-05-21T20:44:48.598680Z","iopub.status.idle":"2025-05-21T20:47:29.362600Z","shell.execute_reply.started":"2025-05-21T20:44:48.598653Z","shell.execute_reply":"2025-05-21T20:47:29.361710Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"claude['claude_3_7_sonnet_20250219_text_response'][1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T20:48:09.712608Z","iopub.execute_input":"2025-05-21T20:48:09.712960Z","iopub.status.idle":"2025-05-21T20:48:09.718847Z","shell.execute_reply.started":"2025-05-21T20:48:09.712906Z","shell.execute_reply":"2025-05-21T20:48:09.717982Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Combine all model outputs into one df\nmerged = pd.merge(llama_deepseek, gpt_4o, on='crisis', how='outer')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T20:48:12.619635Z","iopub.execute_input":"2025-05-21T20:48:12.620027Z","iopub.status.idle":"2025-05-21T20:48:12.631323Z","shell.execute_reply.started":"2025-05-21T20:48:12.620001Z","shell.execute_reply":"2025-05-21T20:48:12.630492Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"experiment_5_results = pd.merge(merged, claude, on='crisis', how='outer')\nexperiment_5_results.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T20:48:13.269504Z","iopub.execute_input":"2025-05-21T20:48:13.269865Z","iopub.status.idle":"2025-05-21T20:48:13.288382Z","shell.execute_reply.started":"2025-05-21T20:48:13.269843Z","shell.execute_reply":"2025-05-21T20:48:13.287508Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"experiment_5_results[['crisis'\n                      ,'issue'\n                      ,'text'\n                      ,'llama3_1_405b_instruct_fp8_text_response'\n                      ,'llama3_1_405b_instruct_fp8_help_text_response'\n                      ,'deepseek_v3_0324_text_response'\n                      ,'deepseek_v3_0324_help_text_response'\n                      ,'chatgpt_4o_latest_text_response'\n                      ,'chatgpt_4o_latest_help_text_response'\n                      ,'claude_3_7_sonnet_20250219_text_response'\n                      ,'claude_3_7_sonnet_20250219_help_text_response']]\nexperiment_5_results.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T20:48:19.051560Z","iopub.execute_input":"2025-05-21T20:48:19.051860Z","iopub.status.idle":"2025-05-21T20:48:19.068978Z","shell.execute_reply.started":"2025-05-21T20:48:19.051832Z","shell.execute_reply":"2025-05-21T20:48:19.067844Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"experiment_5_results.to_csv('experiment_5_results.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T20:48:39.617478Z","iopub.execute_input":"2025-05-21T20:48:39.617751Z","iopub.status.idle":"2025-05-21T20:48:39.632960Z","shell.execute_reply.started":"2025-05-21T20:48:39.617733Z","shell.execute_reply":"2025-05-21T20:48:39.631845Z"}},"outputs":[],"execution_count":null}]}